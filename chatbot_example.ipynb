{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b30969-f839-4618-9537-976987bf6846",
   "metadata": {},
   "source": [
    "## Demo notebook for VA benefits QA agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d7a04-3b9a-4eb6-8edf-f0b5ca172220",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "\n",
    "Data is retrieved from https://www.va.gov/. URLs of specific pages of interest are stored in data/source.py, where the user can add or remove the links as needed.\n",
    "\n",
    "To load the data, we use the parse_html method in src/data_processor.py, which parses the raw html and add section tags \"<title>\", \"<descriptor>\", \"<content>\", and \"<topic>\" to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f572fa6-14b2-4b89-863f-a51d0a8afc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.va.gov/disability/after-you-file-claim/\n",
      "https://www.va.gov/change-address/\n",
      "https://www.va.gov/change-direct-deposit/\n",
      "https://www.va.gov/claim-or-appeal-status/\n",
      "https://www.va.gov/decision-reviews/\n",
      "https://www.va.gov/disability/eligibility/hazardous-materials-exposure/\n",
      "https://www.va.gov/disability/eligibility/illnesses-within-one-year-of-discharge/\n",
      "https://www.va.gov/disability/eligibility/ptsd/\n",
      "https://www.va.gov/disability/eligibility/special-claims/\n",
      "https://www.va.gov/disability/how-to-file-claim/additional-forms/\n",
      "TABLE!!\n",
      "https://www.va.gov/disability/how-to-file-claim/evidence-needed/fully-developed-claims/\n",
      "https://www.va.gov/disability/upload-supporting-evidence/\n",
      "https://www.va.gov/va-payment-history/\n",
      "https://www.va.gov/disability/view-disability-rating/\n",
      "https://www.va.gov/disability/how-to-file-claim/additional-forms/\n",
      "TABLE!!\n",
      "https://www.va.gov/disability/dependency-indemnity-compensation/\n",
      "https://www.va.gov/disability/how-to-file-claim/when-to-file/\n",
      "https://www.va.gov/disability/how-to-file-claim/evidence-needed/\n",
      "https://www.va.gov/disability/how-to-file-claim/\n",
      "https://www.va.gov/disability/eligibility/\n",
      "https://www.va.gov/decision-reviews/supplemental-claim/\n",
      "https://www.va.gov/decision-reviews/higher-level-review/\n",
      "https://www.va.gov/decision-reviews/board-appeal/\n",
      "https://www.va.gov/decision-reviews/after-you-request-review/\n",
      "https://www.va.gov/decision-reviews/contested-claims/\n",
      "https://www.va.gov/decision-reviews/insurance-claims/\n",
      "https://www.va.gov/decision-reviews/fiduciary-claims/\n",
      "https://www.va.gov/disability/effective-date/\n",
      "https://www.va.gov/disability/about-disability-ratings/after-you-get-a-rating/\n",
      "https://www.va.gov/claim-or-appeal-status/\n",
      "https://www.va.gov/decision-reviews/legacy-appeals/\n",
      "https://www.va.gov/resources/decision-reviews-faqs/\n"
     ]
    }
   ],
   "source": [
    "from src.data_processor import parse_html\n",
    "from data.source import URL\n",
    "import re\n",
    "\n",
    "chunks = []\n",
    "for url in URL:\n",
    "    text = parse_html(url)\n",
    "    text = re.sub(\"\\{[^)]*\\}\", \"\", text)\n",
    "    chunks.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c77004-835a-4f18-9c13-f9570f9ae64d",
   "metadata": {},
   "source": [
    "## Build knowledge index\n",
    "\n",
    "Now we can use the text that we've collected from the source urls to construct our knowledge base. This knowledge base is a retrieval index that stores the source for generating our answers.\n",
    "\n",
    "There are several caveats in the process. For the build_index() method, we can configure how we chunk our input texts. I use my custom tags that I created in the first step to annotate some meta data associated with the document body, using the MarkdownHeaderTextSplitter method from langchain. This way even though longer text belonging to a same document is separated into chunks, they are still annotated with the same tag. I also set my chunk size to 500 characters with 100 overlap, as this set up seems to give me the best result for the model I use. However, this is subject to change depending on the nature of your input text, model choice, and prompting strategy.\n",
    "\n",
    "I use 'sentence-transformers/all-MiniLM-L6-v2' as encoder under the hood to obtain the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd9ecca-6f7c-44cd-aa8d-88ed2b8041bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hzhang4/vachat/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.database import Knowledge\n",
    "\n",
    "knowledge = Knowledge(\"VA question answering system\")\n",
    "knowledge.build_index(data=chunks,\n",
    "                     split_headers=[\n",
    "                (\"<topic>\", \"topic\"), (\"<descriptor>\", \"context\")\n",
    "            ],\n",
    "                     chunk_size=500,\n",
    "                     chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813ac97-3173-4a42-b525-3738764a87b3",
   "metadata": {},
   "source": [
    "## Load LLM\n",
    "\n",
    "Then we load the LLM that we want to use in the backend as our question answering agent. I wrapped up with a function load_model() where you can configure which model to use. Since I only tested the app locally, the model has to be pre-downloaded. I store the model in model/\n",
    "\n",
    "Here, we can configure some hyper parameters, such as the temperature for generation, and the maximum number of new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3625d8-21c7-4a68-84f3-9f28b5eb36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm import load_model\n",
    "\n",
    "llm = load_model(path='model/llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "                 model_type='llama',\n",
    "                 max_new_tokens=256,\n",
    "                 temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f0804-daa8-45e2-9454-1659efe9fef2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80ace5e2-ec51-49d9-83f4-1e98e8987485",
   "metadata": {},
   "source": [
    "## Set up the agent\n",
    "\n",
    "Finally we set up the agent. I use the prompt template that I stored in data/prompts.py. Depending on the model, additional prompts can be added/maintained.\n",
    "\n",
    "For the agent to work, we also pass in the knowledge index that has been created earlier. top_k controls how many passages to retrieve for the agent to base their answers off, where I set to 3 as it keeps a balance of sufficient context but not being too long at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5513ee4-5cab-478b-b8dd-6483cb098a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm import agent, set_qa_prompt\n",
    "from data.prompts import qa_template\n",
    "\n",
    "vectorstore = knowledge.get_database()\n",
    "chat_agent = agent(llm=llm, \n",
    "                   vectorstore=vectorstore,\n",
    "                   top_k=3,\n",
    "                   qa_template_=qa_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8b1a3-5523-4c3a-ac62-284a424fb2ad",
   "metadata": {},
   "source": [
    "## Let's chat!\n",
    "\n",
    "And now we can chat with this agent! Since I'm running locally, it may take a while to get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65eb7b5-d243-40e5-bc03-dcea2a2a2966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the review process?',\n",
       " 'result': 'The Higher-Level Review process typically takes an average of 125 days (4 to 5 months) for VA to complete, with the possibility of taking longer if an informal conference is requested as part of the review. If you identify errors in your case during the review process, you can submit a written statement with your application to help VA make a decision faster.',\n",
       " 'source_documents': [Document(page_content='information you want to talk about with the reviewer ready. Prepare to explain any errors in your case.', metadata={'topic': 'How do I ask for an informal conference?'}),\n",
       "  Document(page_content='You can also request a Higher-Level Review by filling\\xa0out a Decision Review Request: Higher-Level Review (VA Form 20-0996). Get VA Form 20-0996 to download Learn more about how to request a Higher-Level Review Note: You can’t submit any evidence. You and/or your representative can speak with the reviewer on the phone. You can tell them why you think the decision should be changed and identify errors. How long does a Higher-Level Review take? Our goal for completing Higher-Level Reviews is an', metadata={'topic': 'Ask for a Higher-Level Review'}),\n",
       "  Document(page_content='<content> How long does a Higher-Level Review take? Our goal for completing Higher-Level Reviews is an average of 125 days (4 to 5 months). Note: If you ask for an informal conference as part of your Higher-Level Review, it may take us longer to complete. Instead, you may want to consider submitting a written statement with your application to tell us about the errors you’ve identified. This will help us make a decision faster.', metadata={'topic': 'How long does a Higher-Level Review take?'})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the review process?\"\n",
    "chat_agent({'query': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a098d26-cb97-4150-a0ef-72427d5a83bb",
   "metadata": {},
   "source": [
    "\n",
    "To make it slightly more user friendly, I created a chainlit app following the same process. You can run **chainlit run ask.py** from the root directory to open up a browser and try it out! \n",
    "\n",
    "However, sometimes the model may generate gibberish. Hopefully it can be improved when more tunings (either through prompting, or hyper parameter setting, or chunking rules) are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df4ab8-96e9-4a39-b617-a119285ba576",
   "metadata": {},
   "source": [
    "### Final thoughts\n",
    "\n",
    "### The choice of chains\n",
    "The chat agent was built with RetrievalQA chain from langchain.chain. I have also experimented with ConversationalRetrievalChain because I wanted to explicitly control how the agent utilizes chat history. However, the generated results seem to be worse with ConversationalRetrievalChain for the initial question as well as follow-ups when context is appended with chat history. Using RetrievalQA chain interactively did not lead to a drastic decrease in answer quality with follow-ups (well, it might have, but the follow-ups that I came up with were somewhat independent from the previous question, as the questions mostly have separate answers from the knowledge index anyways). So the choice of chains was relatively arbituary and the behavior should be evaluated with more thought out questions.\n",
    "\n",
    "### Prompt design\n",
    "It was quite surprising to me since a simple prompt that describes the role of the agent, and the task with a template can already achieve a decent response quality. However, before adding the last sentence in the prompt template (\"Make sure to use complete sentence to answer the question.\") the model would struggle with slightly more complicated questions. It would be interesting to see how the answer might change in a more systematic evaluation on prompt design.\n",
    "\n",
    "### Effect of chunking rule\n",
    "The most dramatic effect on model performance was from how chunking was performed. Slightly shorter chunks with relatively more overlap seems to have improved the generation quality the most. This might be due to the limit of context length for smaller models that I can run locally, and how the input documents are generally structured.\n",
    "\n",
    "Overall, there is still a lot can be done to understand the behavior of this simple llm-based agent. The room for improvement is likely to be pretty big.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
